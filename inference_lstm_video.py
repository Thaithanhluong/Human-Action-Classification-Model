import cv2,os
import mediapipe as mp
import numpy as np
import tensorflow as tf


# Load model
model = tf.keras.models.load_model("./model/model_v1.keras")

# Define actions list
def get_Actions(data_dir="./data"):
    """Láº¥y danh sÃ¡ch cÃ¡c hÃ nh Ä‘á»™ng tá»« thÆ° má»¥c dá»¯ liá»‡u"""
    if not os.path.exists(data_dir):
        print(f"âš ï¸ ThÆ° má»¥c {data_dir} khÃ´ng tá»“n táº¡i! Sá»­ dá»¥ng danh sÃ¡ch máº·c Ä‘á»‹nh.")
        return []
    unique_actions = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
    if not unique_actions:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y hÃ nh Ä‘á»™ng nÃ o trong thÆ° má»¥c dá»¯ liá»‡u!")
    return unique_actions
actions = get_Actions("./data")
if not actions:
    actions = ['UNKNOWN']  # Náº¿u khÃ´ng cÃ³ dá»¯ liá»‡u, Ä‘áº·t nhÃ£n máº·c Ä‘á»‹nh

# print("ðŸ“Œ Danh sÃ¡ch hÃ nh Ä‘á»™ng:", actions)

# Initialize MediaPipe Pose
mp_pose = mp.solutions.pose
pose = mp_pose.Pose()
mp_draw = mp.solutions.drawing_utils

# Open video file
video_path = r"D:\Downloads\video.mp4"
cap = cv2.VideoCapture(video_path)

# Parameters
n_time_steps = 10
lm_list = []
last_prediction = None

# Function to extract keypoints
def extract_keypoints(results):
    keypoints = []
    for lm in results.pose_landmarks.landmark:
        keypoints.append(lm.x)
        keypoints.append(lm.y)
        keypoints.append(lm.z)
        keypoints.append(lm.visibility)
    return keypoints

while cap.isOpened():
    success, img = cap.read()
    if not success:
        break

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    results = pose.process(img_rgb)

    if results.pose_landmarks:
        keypoints = extract_keypoints(results)
        lm_list.append(keypoints)

        # Náº¿u sá»‘ lÆ°á»£ng frame vÆ°á»£t quÃ¡ n_time_steps thÃ¬ xÃ³a frame cÅ© nháº¥t
        if len(lm_list) > n_time_steps:
            lm_list.pop(0)

        # Chá»‰ dá»± Ä‘oÃ¡n khi Ä‘Ã£ Ä‘á»§ n_time_steps frame
        if len(lm_list) == n_time_steps:
            input_data = np.expand_dims(np.array(lm_list), axis=0)
            predictions_array = model.predict(input_data, verbose=0)
            max_index = np.argmax(predictions_array)
            confidence = predictions_array[0][max_index] * 100
            current_prediction = f"{actions[max_index]}"

            # Chá»‰ in náº¿u dá»± Ä‘oÃ¡n khÃ¡c láº§n trÆ°á»›c
            if current_prediction != last_prediction:
                print(f"Predicted action: {current_prediction}")
                last_prediction = current_prediction

cap.release()
